Modern generative AI solutions for generating music are very promising, but they lack connection to actual problems faced by music producers and artists. This project builds on state-of-the-art latent diffusion models for text-to-audio (TTA) generation like AudioLDM with the aim of creating short, high-fidelity instrumental samples that can be used to create virtual instruments. To achieve this, we will train a conditional LDM on mel spectrograms, using our variational auto-encoder to create diverse samples, which are necessary for creating realistic virtual instruments. We will use small input spectrograms and small-dimensional latents to decrease inference times, creating a model that is a robust and efficient tool for music producers.



More information on this project coming soon...