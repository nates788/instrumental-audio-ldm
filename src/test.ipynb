{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "torch.Size([2, 4629])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pipeline\n",
    "import model_loader\n",
    "import numpy as np\n",
    "#from tqdm import tqdm\n",
    "#from ddpm import DDPMSampler\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "#from diffusion import Diffusion\n",
    "#from encoder import VAE_Encoder\n",
    "#from decoder import VAE_Decoder\n",
    "import pandas as pd\n",
    "#from transformers import CLIPTokenizer\n",
    "import os\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "#torch.set_default_device(\"mps\")\n",
    "\n",
    "AUDIO_PATH = \"/Users/nathanielsmith/Desktop/ai/audio_diffusion/audio/\"\n",
    "LABELS_FILE_PATH = AUDIO_PATH + \"labels.csv\"\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "INPUT_AUDIO_LENGTH_SECONDS = 1\n",
    "\n",
    "#DEVICE = torch.device(\"mps:0\")\n",
    "\n",
    "dataset = torch.randn(0, 2, 65536)\n",
    "desired_dimensions = [2, 65536]\n",
    "\n",
    "data = pd.read_csv(LABELS_FILE_PATH, names=[\"id\", \"label\", \"audio_path\"])\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "for path in data[\"audio_path\"]:\n",
    "    waveform, sample_rate = torchaudio.load(AUDIO_PATH + path)\n",
    "    print(waveform.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"../data/v1-5-pruned-emaonly.ckpt\"\n",
    "#models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "\n",
    "def get_time_embedding(timestep):\n",
    "    # Shape: (160,)\n",
    "    freqs = torch.pow(10000, -torch.arange(start=0, end=160, dtype=torch.float32) / 160) \n",
    "    # Shape: (1, 160)\n",
    "    x = torch.tensor([timestep], dtype=torch.float32)[:, None] * freqs[None]\n",
    "    # Shape: (1, 160 * 2)\n",
    "    return torch.cat([torch.cos(x), torch.sin(x)], dim=-1)\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 128  # the generated image resolution\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  # how many images to sample during evaluation\n",
    "    num_epochs = 1 #50\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"ddpm-butterflies-128\"  # the model name locally and on the HF Hub\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
    "    hub_model_id = \"<your-username>/<my-awesome-model>\"  # the name of the repository to create on the HF Hub\n",
    "    hub_private_repo = False\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "tokenizer = CLIPTokenizer(\"../data/vocab.json\", merges_file=\"../data/merges.txt\")\n",
    "#clip = models[\"clip\"]\n",
    "\n",
    "# load dataset\n",
    "\n",
    "#dataset = torch.randn(0, 2, 44100)\n",
    "#desired_dimensions = [2, 44100]\n",
    "\n",
    "dataset = torch.randn(0, 2, 65536)\n",
    "desired_dimensions = [2, 65536]\n",
    "\n",
    "data = pd.read_csv(LABELS_FILE_PATH, names=[\"id\", \"label\", \"audio_path\"])\n",
    "\n",
    "for path in data[\"audio_path\"]:\n",
    "    waveform, sample_rate = torchaudio.load(AUDIO_PATH + path)\n",
    "\n",
    "    if waveform.size(1) < INPUT_AUDIO_LENGTH_SECONDS * SAMPLE_RATE:\n",
    "        padding_width = desired_dimensions[1] - waveform.shape[1]\n",
    "        waveform = F.pad(waveform, (0, padding_width), mode='constant', value=0)\n",
    "    elif waveform.size(1) > INPUT_AUDIO_LENGTH_SECONDS * SAMPLE_RATE:\n",
    "        waveform = waveform[:, :desired_dimensions[1]]\n",
    "    \n",
    "\n",
    "    waveform = waveform.unsqueeze(0)\n",
    "\n",
    "    dataset = torch.cat([dataset, waveform], dim=0)\n",
    "\n",
    "\n",
    "# preprocess data\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "encoder = VAE_Encoder()\n",
    "diffusion = Diffusion()\n",
    "decoder = VAE_Decoder()\n",
    "\n",
    "encoder.to(DEVICE)\n",
    "diffusion.to(DEVICE)\n",
    "decoder.to(DEVICE)\n",
    "\n",
    "\n",
    "loss_function = torch.nn.MSELoss()  # Choose an appropriate loss function\n",
    "optimizer = torch.optim.AdamW(list(encoder.parameters()) + list(diffusion.parameters()) + list(decoder.parameters()), lr=config.learning_rate)\n",
    "\n",
    "generator = torch.Generator(device=DEVICE)\n",
    "\n",
    "\n",
    "noise_scheduler = DDPMSampler(generator)\n",
    "noise_scheduler.set_inference_timesteps(50)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(config.num_epochs):\n",
    "    progress_bar = tqdm(total=len(train_dataloader))\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "    for batch_data in train_dataloader:\n",
    "        # Forward pass\n",
    "        input_data = batch_data  # Adjust as needed, make sure these are just the audios/images\n",
    "        #labels = input_data\n",
    "        input_data.to(DEVICE)\n",
    "        #context = torch.randn(1, 77, batch_data.size(2))\n",
    "        context = torch.randn(1, 77, 768, device=DEVICE)\n",
    "\n",
    "\n",
    "        #tokens = tokenizer.batch_encode_plus(\n",
    "        #    [label], padding=\"max_length\", max_length=77\n",
    "        #).input_ids\n",
    "        #tokens = torch.tensor(tokens, dtype=torch.long, device=DEVICE)\n",
    "        #context = clip(tokens)\n",
    "\n",
    "        # Generate noise\n",
    "        encoder_noise = torch.randn([1, 4, 8192], device=DEVICE).to(DEVICE)\n",
    "\n",
    "        bs = input_data.shape[0]\n",
    "\n",
    "        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=DEVICE,dtype=torch.int64)\n",
    "        latents = encoder(input_data, encoder_noise)\n",
    "        print(\"a: \", latents.shape)\n",
    "        latents = noise_scheduler.add_noise(latents, noise_scheduler.timesteps[0])\n",
    "        print(\"b: \", latents.shape)        \n",
    "        timesteps = tqdm(noise_scheduler.timesteps)\n",
    "        for i, timestep in enumerate(timesteps):\n",
    "            time_embedding = get_time_embedding(timestep).to(DEVICE)\n",
    "\n",
    "            model_input = latents\n",
    "            print(\"a\")\n",
    "            model_output = diffusion(model_input, context, time_embedding)\n",
    "            print(\"b\")\n",
    "            latents = noise_scheduler.step(timestep, latents, model_output)\n",
    "            print(\"c\")\n",
    "        print(\"c: \", latents.shape)        \n",
    "\n",
    "\n",
    "        output = decoder(latents)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(output, input_data)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print epoch loss or other metrics\n",
    "    print(f'Epoch [{epoch+1}/{config.num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "torch.save({\n",
    "    'encoder_state_dict': encoder.state_dict(),\n",
    "    'diffusion_state_dict': diffusion.state_dict(),\n",
    "    'decoder_state_dict': decoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, '../data/data.pth')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
